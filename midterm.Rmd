---
title: "Midterm_markdown"
output: 
  html_document:
    code_folding: hide
date: "2023-10-11"
author: "Sihan Ren, Jingyi Li"
editor_options: 
  markdown: 
    wrap: 72

---
# I. Introduction
As the housing market is highly replied on the price predictions for both sellers and buyers, it is essential to make a highly accurate prediction model for them. Zillow is an American real estate company that has a large database of homes for sale and rent. However, because the range of their data is across the United States, it is hard for them to make a precise housing price prediction in a specific city. The purpose of the project is to help Zillow to make a prediction model of the housing price in Philadelphia based on the local characteristics. The model would provide a strong guide for sellers and buyers to make decisions so that they would not overpay much. It would also reduce the risks of credibility and enhance the market efficiency. 

The overall modeling strategy for the project is using Ordinary Least Squares (OLS) regression with multi-variables, which is a popular statistical method used to examine the relationship between a dependent variable and predictors that might affect the value of the dependent variable. The method provides the strength of the relationship, the direction of the relationship and the goodness of the model fit. In our case, there are still some challenges we should overcome. First of all, we need to filter out the most significant and diverse variables to use in this model and find the data from the reliable sources. Second, we need to provide strong evidence and clear visualizations of plots and maps with comprehensive interpretations for the audience. Third, we should state the limitations of the OLS regression results. There are still unpredictable factors in the real world, such as Covid-19 pandemic, which impact the housing prices negatively.

Our results of the model indicates that using the local data can enhance the accuracy of housing price predictions for Philadelphia. 

# II. Data Wrangling
To decide the specific variables that might affect the housing prices, we start from three categories: interior conditions, neighborhood environments, and demographic characteristics. According to Zillow's website, the living area, the house age and utilities are listed in the overview section in each house's information, which are the top factors that people consider the most when they buy a house. Therefore, we chose the column "total_area", "year_built", "quality_grade" in the original dataset to represent these three factors. For the environmental factors, people normally consider the safety and convenience to the public resources, so we explored the OpenDataPhilly website and used the data of shooting victims, commercial corridors and schools. For the demographics characteristics, we retrieved the census data in Philadelphia in 2021 from U.S. Census Bureau, which contains the median household income, percentage of foreign-born residents, and the geographic mobility to a different house.

## 2.1 Import packages and start the setting
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
options(kableExtra.auto_format = FALSE)
```

```{r setup_packages, warning = FALSE, message = FALSE}
# Load Libraries
library(geos)
library(rsgeo)
library(tidyverse)
library(tidycensus)
library(sf)
library(kableExtra)
library(knitr)
library(dplyr)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot)
library(stargazer)


options(scipen=999)
options(tigris_class = "sf")

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")
```

## 2.2 Import and clean the general dataset
Since our project is heavily focused on geographic features, we retrieved the block groups and neighborhoods boundaries from OpenDataPhilly, and we filtered the variables that we need to use from the housing price raw data.

```{r load_key, echo = TRUE, warning=FALSE, include=FALSE, results='hide'}
census_api_key("6a81f5ae68fcb8e26d2f0a80f4232c5e503b553d", overwrite = TRUE)

Philly_block_groups <-
  st_read("https://opendata.arcgis.com/datasets/2f982bada233478ea0100528227febce_0.geojson") %>%
  st_transform('ESRI:102728')

neighborhood <- st_read("/Users/rachelren/Desktop/Upenn/MUSA_5080/Midterm/MUSA508_Midterm_Li_Ren/Data/Neighborhoods_Philadelphia.geojson") %>%
  st_transform('ESRI:102728')
  
Philly_price_all <-
  st_read("/Users/rachelren/Desktop/Upenn/MUSA_5080/Midterm/MUSA508_Midterm_Li_Ren/Data/studentData.geojson") %>%
  st_transform('ESRI: 102728') 

Philly_price_clean <- Philly_price_all[, c("objectid", "census_tract", "total_area", "interior_condition", "sale_price", "year_built", "geometry", "toPredict")] ## Select columns we use

## Calculate the age of the house
Philly_price_clean <- 
  Philly_price_clean %>%
  mutate(houseAge = ifelse((year_built > 0 & year_built < 2023), 2023-year_built, 0))

## Data of the housing nearby environments
Shooting_victims <-
  st_read("/Users/rachelren/Desktop/Upenn/MUSA_5080/Midterm/MUSA508_Midterm_Li_Ren/Data/shootings.geojson") %>%
  st_transform('ESRI: 102728')

PPR_Sites <-
  st_read("/Users/rachelren/Desktop/Upenn/MUSA_5080/Midterm/MUSA508_Midterm_Li_Ren/Data/PPR_Program_Sites.geojson") %>%
  st_transform('ESRI: 102728')

Commercial_Corridors <-
  st_read("/Users/rachelren/Desktop/Upenn/MUSA_5080/Midterm/MUSA508_Midterm_Li_Ren/Data/Commercial_Corridors.geojson") %>%
  st_transform('ESRI: 102728')

schools <- 
  read.csv("/Users/rachelren/Desktop/Upenn/MUSA_5080/Midterm/MUSA508_Midterm_Li_Ren/Data/Schools.csv")

schools.sf <- 
  schools %>% 
  st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102728')

##Get and clean the variables in the 2021 Census data
## all of the variables are in block group
blockgroup21 <-  
  get_acs(geography = "block group",
          variables = c("B19013_001E","B99051_005E",
                        "B01003_001E","B07201_003E",
                        "B07201_001E"), 
          year=2021, state=42,
          county=101, geometry=TRUE) %>% 
  st_transform('ESRI:102728')

variables2021 <- load_variables(2021,'acs5')
variables2021_blockGroup <- load_variables(2021,'acs5') %>% 
dplyr::filter(geography == 'block group')

variables2021_tract <- load_variables(2021,'acs5') %>% 
dplyr::filter(geography == 'tract')

blockgroup21 <- 
  blockgroup21 %>%
  dplyr::select( -NAME, -moe) %>%
  spread(key = variable, value = estimate) %>%
  rename(TotalPop = B01003_001, 
         ForeignBorn = B99051_005,
         MedHHInc = B19013_001, 
         MobilityDifferentHouse = B07201_003,
         MobilityTotal = B07201_001
         )
## Calculate the percentage of foreign born and the mobility rate
blockgroup21 <- 
  blockgroup21 %>%
  mutate(pctForeign = ifelse(TotalPop > 0, ForeignBorn / TotalPop, 0),
         pctMobility = ifelse(MobilityTotal > 0, (MobilityDifferentHouse / MobilityTotal), 0),
         year = "2021") %>%
  dplyr::select(-ForeignBorn,-MobilityDifferentHouse,-MobilityTotal,-TotalPop)
```

In order to better apply the environmental impact to the housing prices, we use two different ways. For the shooting data, we selected the number of victims who suffered the gun shot on head within 1000 feet around each house because the head wound is the most dangerous body part that caused the death. Also, because the location and time of the crime is random, a certain buffer is appropriate to measure the safety levels in a community. Even a fatal crime in a relatively far distance can cause the panic and affect the housing price. Therefore, we choose the buffer to sum crimes rather than using the average nearest neighbor distance.

```{r echo = TRUE, warning=FALSE, include=FALSE, results='hide'}
Shooting_victims %>% 
group_by(wound) %>%
  summarize(count = n()) %>%
  arrange(-count)

PhillyShootingHead.sf <-
  Shooting_victims %>%
    filter(wound == "Head",
           point_y > -1) %>%
    dplyr::select(point_y, point_x) %>%
    na.omit() %>%
    st_as_sf(coords = c("Long", "Lat")) %>%
    st_transform('ESRI:102728') %>%
    distinct()

Philly_price_clean$shooting.Buffer <- Philly_price_clean %>% 
    st_buffer(1000) %>% 
    aggregate(mutate(PhillyShootingHead.sf, counter = 1),., sum) %>%
    pull(counter)
```

The second way is applied to schools, commercial corridors and public parks and recreational centers. We used the method of calculating the Euclidean distance this time because the distance to these public resources are much more important. It is used to represent the convenience of living in a house. The variables for the further analysis are the distances from each house to its nearest public amenity.

```{r echo = TRUE, warning=FALSE, results = 'hide'}
nearest_school <- sf::st_nearest_feature(Philly_price_clean, schools.sf)
x <- rsgeo::as_rsgeo(Philly_price_clean)
y <- rsgeo::as_rsgeo(schools.sf)

Philly_price_clean$dist_to_school <- rsgeo::distance_euclidean_pairwise(x, y[nearest_school])

nearest_Commercial <- sf::st_nearest_feature(Philly_price_clean, Commercial_Corridors)
x <- rsgeo::as_rsgeo(Philly_price_clean)
y <- rsgeo::as_rsgeo(Commercial_Corridors)
Philly_price_clean$dist_to_commerce <- rsgeo::distance_euclidean_pairwise(x, y[nearest_Commercial])

nearest_PPR <- sf::st_nearest_feature(Philly_price_clean, PPR_Sites)
x <- rsgeo::as_rsgeo(Philly_price_clean)
y <- rsgeo::as_rsgeo(PPR_Sites)
Philly_price_clean$dist_to_PPR <- rsgeo::distance_euclidean_pairwise(x, y[nearest_PPR])

```

For the census variables, we simply joined the data of each block groups to the houses within that block group. We also assigned the neighborhood of each house for the future use and divide the quality condition into four parts: good condition - 4, fair condition - 3, bad condition - 2, others - 1

```{r echo = TRUE, warning=FALSE, results = 'hide'}
Philly_Housing_joined <-
  st_join(Philly_price_clean, blockgroup21, join = st_within)
Philly_Housing_joined <-
  st_join(Philly_Housing_joined, neighborhood, join = st_within)

```

### A table of summary statistics with variable descriptions
Table 2.1 gives us a comprehensive summary of the independent variables that might affect the housing prices. The total number of the sample is 23781. Here is a descriptions of the variables:

House condition:
  - total_area: the area that the owners have, which includes the living area and the outdoor area
  - houseAge: the age of the house since it was first built.
  
Demographic characteristics:
  - MedHHInc: the median household income in the past 12 months (in 2021 inflation-adjusted dollars)
  - pctForeign: the percentage of residents who were born outside of the United States among all residents
  - pctMobility: the percentage of geographic mobility in a different house in United States 1 year ago
  
Environmental impact:
  - shooting.Buffer: the sum of victims suffered from head gun shot within a 1000 feet buffer of each house
  - dist_to_school: the distance of a house to the nearest school
  - dist_to_commerce: the distance of a house to the nearest commercial corridor
  - dist_to_PPR: the distance of a house to the nearest public park and recreational center
  
The table calculates the maximum, minimum, mean, and standard deviation of each variable, which indicates a large variability of the data from the standard deviation. We need to consider if there are any outliers for the future analysis.

```{r summary_table, echo=TRUE}
numeric_columns <- sapply(Philly_Housing_joined, is.numeric)
for (col in names(Philly_Housing_joined)[numeric_columns]) {
  col_mean <- mean(Philly_Housing_joined[[col]], na.rm = TRUE)
  Philly_Housing_joined[[col]][is.na(Philly_Housing_joined[[col]])] <- col_mean
}

house_condition <- st_drop_geometry(Philly_Housing_joined) %>%
  select(total_area, houseAge, interior_condition) %>%
  gather(variable, value) %>%
  group_by(variable) %>%
  summarize(
    mean = mean(value, na.rm = TRUE),
    sd = sd(value, na.rm = TRUE),
    Min = min(value, na.rm = TRUE),
    Max = max(value, na.rm = TRUE)
  )

demo <- st_drop_geometry(Philly_Housing_joined) %>%
  select(MedHHInc, pctForeign, pctMobility) %>%
  gather(variable, value) %>%
  group_by(variable) %>%
  summarize(
    mean = mean(value, na.rm = TRUE),
    sd = sd(value, na.rm = TRUE),
    Min = min(value, na.rm = TRUE),
    Max = max(value, na.rm = TRUE)
  )

envi_impact <- st_drop_geometry(Philly_Housing_joined) %>%
  select(shooting.Buffer, dist_to_school, dist_to_commerce, dist_to_PPR) %>%
  gather(variable, value) %>%
  group_by(variable) %>%
  summarize(
    mean = mean(value, na.rm = TRUE),
    sd = sd(value, na.rm = TRUE),
    Min = min(value, na.rm = TRUE),
    Max = max(value, na.rm = TRUE)
  )

summary_stats <- rbind(house_condition, demo, envi_impact)  
summary_table <- knitr::kable(summary_stats, "html", digits = 1, caption = "Table of summary statistics") %>%
  kable_styling(full_width = F)

grouped_table <- group_rows(summary_table, "House Condition", 1, 3)
grouped_table <- group_rows(grouped_table, "Demographic Characteristics", 4, 6)
grouped_table <- group_rows(grouped_table, "Environmental Impact", 7, 10)

grouped_table

```

### Correlation Matrix across variables
Figure 2.1 is a correlation matrix among the numeric variables, which represents the pairwise degree of correlation between two variables in x-axis and y-axis. The correlation coefficient represents the positive, negative, or no correlation. A positive correlation means that when the value of one variable increases, the value of the other variable increases and vice versa. 0 means no correlation, and the change in one variable do not predict changes in the other. Most of the variable pairs have a 0 correlation or a slightly negative correlation, but few pairs, such as distance to school with distance to parks, are highly positively correlated.

```{r Corre Matrix}
numericVars <- st_drop_geometry(Philly_Housing_joined)[, c("total_area", "houseAge", "interior_condition", "MedHHInc", "pctForeign", "pctMobility", "shooting.Buffer", "dist_to_school", "dist_to_commerce", "dist_to_PPR")]

ggcorrplot(
  round(cor(numericVars), 1),
  p.mat = cor_pmat(numericVars),
  colors = c("#25CB10", "white", "#FA7800"),
  type="lower",
  insig = "blank") +
    labs(title = "Correlation matrix across variables", 
         label = "Figure 2.1")

```

### Home price correlation scatterplot with median household income
This scatterplot (Figure 2.2) indicates a relationship between the home price and the median household income. The blue line is a line of best fit, which has a positive but slow slope. As the median household income increases, the house price also increases, but it would not increase a lot. Points are also concentrated in the bottom left corner, meaning that households with lower income tend to live in houses with lower prices. However, the data is more scattered for households with high income. Their home prices vary across the y-axis. There are also three outliers which have a price near 600 millions.
```{r}
ggplot(Philly_Housing_joined, aes(x = MedHHInc, y = sale_price)) +
  geom_point(size = .5) +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(
    title = "Price as a function of median household income",
    x = "Median Household Income",
    y = "Home Price",
    label = "Figure 2.2"
  ) +
  theme_minimal()
```
### Home price correlation scatterplot with house age
Figure 2.3 is a scatterplot showing the correlation between the house ages and the house prices. The best fit line shows that as the house ages increase, the house prices gently decreases. There is also a large variety in the home price at a specific house age. We might consider if house age is a good predictor of the house value.
```{r}
ggplot(Philly_Housing_joined, aes(x = houseAge, y = sale_price)) +
  geom_point(size = .5) +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(
    title = "Price as a function of house age",
    x = "Age",
    y = "Home Price",
    label = "Figure 2.3"
  ) +
  theme_minimal()
```
### Home Price Correlation Scatterplot with total areas
Figure 2.4 shows the correlation between the total areas of the house and the house price. There is a strong positive relationship as the slope of the best fit line is greater than the previous two plots. The house prices increase as the total areas are larger. However, there is still a variety of house prices when the total areas are relatively small. For example, there are some outliers that a small area of house has significant highest prices.
```{r}
ggplot(Philly_Housing_joined, aes(x = total_area, y = sale_price)) +
  geom_point(size = .5) +
  geom_smooth(method = "lm", se = FALSE, color = "orange") +
  labs(
    title = "Price as a function of total areas",
    x = "total areas",
    y = "Home Price",
    label = "Figure 2.4"
  ) +
  theme_minimal()
```
### Home Price Correlation with Distance to Parks and Recreation
Figure 2.5 shows the correlation between the distance to the nearest parks and recreation and the house price. The slope of the best fit line is not obvious, which is almost horizontal to the x-axis. It means that no matter how far the distances to parks and recreation center change, the house prices are not affected by them too much.
```{r}
ggplot(Philly_Housing_joined, aes(x = dist_to_PPR, y = sale_price)) +
  geom_point(size = .5) +
  geom_smooth(method = "lm", se = FALSE, color = "orange") +
  labs(
    title = "Price as a function of distance to Parks and Recreation",
    x = "Distance to Parks and Recreation",
    y = "Home Price",
    label = "Figure 2.5"
  ) +
  theme_minimal()
```
### Develop 1 map of dependent variable (sale price)
Figure 2.6 is a choropleth map of the sales price distribution by block groups in Philadelphia. The housing value is grouped in equal intervals, and lighter blue represents the lower sale prices, while the darker blue represents the higher sale prices. Most of the neighborhoods have a relative low prices less than 1000000. Prices are higher in the northwest block groups as well as the downtown areas.


```{r}
mean_sale_price <- Philly_Housing_joined %>%
  group_by(GEOID) %>%
  summarize(MeanSalePrice = mean(sale_price))

blockgroup21 <- st_join(blockgroup21, mean_sale_price, by = "GEOID")
blockgroup21$MeanSalePrice[is.na(blockgroup21$MeanSalePrice)] <- 0
ggplot() +
  geom_sf(data = blockgroup21, aes(fill = MeanSalePrice)) +
  scale_fill_gradient(low = "lightblue", high = "darkblue", 
                      name = "Sale Price (U.S. Dollars)") +
  labs(title = "Choropleth Map of Housing Sale Price by neighborhoods",
       label = "Figure 2.6") +
  theme_minimal()
```
### Develop a map of most interesting independent variables: Geographic Mobility
The map of the geographic mobility by block groups shows a scattered pattern. There is no cluster that several adjacent block groups have the highest or the lowest mobility rate. However, there is one block group in the dark blue which has a significant high mobility rate. 

```{r}
ggplot() +
  geom_sf(data = blockgroup21, aes(fill = pctMobility)) +
  scale_fill_gradient(low = "lightblue", high = "darkblue", name = "Geographic Mobility Rate") +
  labs(title = "Choropleth Map of Geographic Mobility by Block Groups",
       label = "Figure 2.7") +
  theme_minimal()
```
### Develop map of most interesting independent variables: School Density
The map shows the average distance to the nearest school by block groups. It is obvious to see that the south Philadelphia has a shorter average distance and the distance is longer in the most blocks of the north Philadelphia. Block groups in downtown Philadelphia have similar distances from 2000 to 3000 feet.

```{r}
mean_school_dis <- Philly_Housing_joined %>%
  group_by(GEOID) %>%
  summarize(MeanSchoolDis = mean(dist_to_school))

blockgroup21 <- st_join(blockgroup21, mean_school_dis, by = "GEOID")
blockgroup21$MeanSchoolDis[is.na(blockgroup21$MeanSchoolDis)] <- 0
ggplot() +
  geom_sf(data = blockgroup21, aes(fill = MeanSchoolDis)) +
  scale_fill_gradient(low = "lightblue", high = "darkblue", name = "Average Distance") +
  labs(title = "Choropleth Map of Distance to school by Block Groups",
       label = "Figure 2.8") +
  theme_minimal()
```

### Develop a map of most interesting independent variable: Shooting Victims with Head Injuries
The heat map shows the general density of victims who suffered from head gun shot wounds. It is clear to see that the incidents happened mostly in the downtown and west of Philadelphia. 

```{r}
ggplot() + geom_sf(data = blockgroup21, fill = "grey40") +
  stat_density2d(data = data.frame(st_coordinates(PhillyShootingHead.sf)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_gradient(low = "lightblue", high = "purple", name = "Density") +
  scale_alpha(range = c(0.00, 0.35), guide = "none") +
  labs(title = "Density map of Shootings in head distribution, Philadelphia",
       label = "Figure 2.9") +
  mapTheme()

```
## III. Methods
### 3.1 Coding techniques:
We made our OLS regression analysis in R programming language. The use of packages helped us create data visualizations and statistical calculation, including kable, ggplot, stargazer, etc. We combined our code and report into an R markdown file, which can be knitted into an html file eventually.

### 3.2 Data Analysis:
#### Data integration:
In part II, we introduced our datasets and provided a general summary of the statistics of our predictors. The correlation matrix, scatterplots and geospatial visualization created a multidimensional way to understand the predictors. A comprehensive table of summary statistics was produced to provide insights into the distribution of all variables in the dataset. These variables were sorted into categories for easy reference: internal characteristics, amenities/public services, and spatial structure.

#### OLS Regression analysis
First of all, we separated the data which contains the actual sale prices with the data which needs to be predicted. For the modelling data, we split it into training and test sets in the proportion of 3:2. The training data is for the machine learning algorithm to learn and make the model with the minimum error, while the test data is used to evaluate the model's performance for predicting the house values. Therefore, we made a linear regression model using the lm function in R, and the values in the model will be interpreted in the Results section. Mean absolute error is also calculated for the test set to measure the accuracy of the model. 

Second, we went through a 100-fold cross validation to enhance the model's performance. It split the test data into 100 parts equally and randomly to train on k-1 of the folds repeatedly. To interpret the mean absolute error and the correlation between the predicted results and the actual sale prices in a more visualized way, we created a histogram plot, a scattered plot, and a map to show its distribution.

Third, we did a Moran's I test to test the spatial autocorrelation. The plot shows the value of Moran's I and the frequency of 999 random permutated I. A scatterplot of spatial lag of errors and prediction errors is also displayed. It is used to show if the spatial autocorrelation in the errors exist. With the above model, we put our unpredicted data into the regression model and showed the results in a map.

The last part of the analysis is the generalizability of neighborhood model, which is to test the generalizability across the space. To achieve this, we first ran another regression by each neighborhood and created a map to visualize the mean absolute percent errors by neighborhood. We also did a testing using the Census data of median household income and foreign-born residents to categorize the neighborhoods and making tables to compare the mean absolute percent errors.

## Results
### Training set lm summary results
```{r}
Philly_price_Model <- Philly_Housing_joined %>%
  filter(toPredict == "MODELLING")

inTrain <- createDataPartition(
              y = paste(Philly_price_Model$interior_condition), 
              p = .60, list = FALSE)
PhillyPrice.training <- Philly_price_Model[inTrain,] 
PhillyPrice.test <- Philly_price_Model[-inTrain,]  
 
reg.training <-
  lm(sale_price ~ ., data = as.data.frame(PhillyPrice.training) %>%
                             dplyr::select(sale_price, total_area, interior_condition, houseAge, shooting.Buffer, dist_to_school, dist_to_commerce, dist_to_PPR, MedHHInc, pctForeign, pctMobility))

summary_lm <- summary(reg.training)

# Create a table of the model summary using stargazer
stargazer(reg.training,
          title = "Linear Regression Summary",
          type = "text",
          align = TRUE
          )

```
###  table of mean absolute error and MAPE for a single test set
```{r}
PhillyPrice.test <-
  PhillyPrice.test %>%
  mutate(sale_price.Predict = predict(reg.training, PhillyPrice.test),
         sale_price.Error = sale_price.Predict - sale_price,
         sale_price.AbsError = abs(sale_price.Predict - sale_price),
         sale_price.APE = (abs(sale_price.Predict - sale_price)) / sale_price.Predict)

error_summary <- st_drop_geometry(PhillyPrice.test) %>%
  summarize(
    MAE = mean(sale_price.AbsError),
    MAPE = mean(sale_price.APE)
  )

kable(error_summary, "html", caption = "Error Summary for Test Set", digits = 2) %>%
  kable_styling()
```

### Cross validation tests with 100 folds
```{r}
fitControl <- trainControl(method = "cv", number = 100)
set.seed(825)

reg.cv <- 
  train(sale_price ~ ., data = st_drop_geometry(Philly_price_Model) %>% 
                                dplyr::select(sale_price, 
                                total_area, interior_condition, 
                                houseAge, shooting.Buffer, dist_to_school, 
                                dist_to_commerce, dist_to_PPR, MedHHInc, 
                                pctForeign, pctMobility), 
     method = "lm", trControl = fitControl, na.action = na.pass)

reg.cv
```

### Calculate the mean and standard deviation of MAE
```{r}
mean(reg.cv$resample[,3])
sd(reg.cv$resample[,3])
```
### Plot histogram of MAE
```{r}
cv_results <- data.frame(Resamples = names(reg.cv$resample),
                         MAE = reg.cv$resample$MAE)

# Create a histogram of MAE values
ggplot(cv_results, aes(x = MAE)) +
  geom_histogram(binwidth = 1000, fill = "blue", color = "black") +
  labs(title = "Cross-Validation Mean Absolute Error (MAE) Histogram",
       x = "Mean Absolute Error (MAE)",
       y = "Frequency")
```

### Plot predicted prices as a function fo observed prices
```{r}
scatter_plot <- ggplot(data = PhillyPrice.test, aes(x = sale_price.Predict, y = sale_price)) +
  geom_point() +  # Add scatter points
  geom_abline(intercept = 0, slope = 1, color = "orange", linetype = "dashed") +  # Perfect fit line
  geom_abline(intercept = mean(PhillyPrice.test$sale_price), slope = 1, color = "green", linetype = "dashed") +  # Average predicted fit line
  labs(title = "Scatter Plot of SalePrice vs. SalePrice.Predict",
       x = "SalePrice.Predict",
       y = "SalePrice")

# Print the scatter plot
print(scatter_plot)
```
### Provide a map of your residuals for your test set.
```{r}
palette5 <- c("#25CB10", "#5AB60C", "#8FA108",   "#C48C04", "#FA7800")
ggplot() +
  geom_sf(data = blockgroup21, fill = "grey40") +
  geom_sf(data = PhillyPrice.test, aes(colour = q5(sale_price.Error)), 
          show.legend = "point", size = .75) +
  scale_colour_manual(values = palette5,
                   name="Quintile\nBreaks") +
  labs(title="Map of Residuals of test set") +
  mapTheme()
```
### Moran's I test
```{r}
coords <- st_coordinates(Philly_price_Model) 

neighborList <- knn2nb(knearneigh(coords, 5))

spatialWeights <- nb2listw(neighborList, style="W")

Philly_price_Model$lagPrice <- lag.listw(spatialWeights, Philly_price_Model$sale_price)

coords.test <-  st_coordinates(PhillyPrice.test) 

neighborList.test <- knn2nb(knearneigh(coords.test, 5))

spatialWeights.test <- nb2listw(neighborList.test, style="W")

PhillyPrice.test <- PhillyPrice.test %>% 
  mutate(lagPriceError = lag.listw(spatialWeights.test, sale_price.Error))

moranTest <- moran.mc(PhillyPrice.test$sale_price.Error, 
                      spatialWeights.test, nsim = 999)

ggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = moranTest$statistic), colour = "#FA7800",size=1) +
  scale_x_continuous(limits = c(-1, 1)) +
  labs(title="Observed and permuted Moran's I",
       subtitle= "Observed Moran's I in orange",
       x="Moran's I",
       y="Count") +
  plotTheme()
```
### Scatter plot of error as a function of the spatial lag of price
```{r}
ggplot(PhillyPrice.test, aes(x = lagPriceError, y = sale_price.Error)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "orange")+
  labs(title = "Error as a function of the spatial lag of errors",
       x = "lagPriceError",
       y = "sale_price.Error")
```
### Provide a map of your predicted values for where ‘toPredict’ is both “MODELLING” and “CHALLENGE”.
```{r}
Philly_Housing_joined <-
  Philly_Housing_joined %>%
  mutate(sale_price.Predict = predict(reg.training, Philly_Housing_joined))
```

```{r}
ggplot() +
  geom_sf(data = neighborhood, fill = "grey40") +
  geom_sf(data = Philly_Housing_joined, aes(colour = q5(sale_price.Predict)), 
          show.legend = "point", size = .75) +
  scale_colour_manual(values = palette5,
                   name="Quintile\nBreaks") +
  labs(title="Map of predicted values") +
  mapTheme()
```
### Using the test set predictions, provide a map of mean absolute percentage error (MAPE) by neighborhood.
```{r}
left_join(
  st_drop_geometry(PhillyPrice.test) %>%
    group_by(name) %>%
    summarize(meanPrice = mean(sale_price, na.rm = T)),
  mutate(PhillyPrice.test, predict.fe =
                        predict(lm(sale_price ~ name, data = PhillyPrice.test),
                        PhillyPrice.test)) %>%
    st_drop_geometry %>%
    group_by(name) %>%
      summarize(meanPrediction = mean(predict.fe)))

reg.nhood <- lm(sale_price ~ ., data = as.data.frame(PhillyPrice.training) %>% 
                                 dplyr::select(sale_price, total_area, interior_condition, houseAge, shooting.Buffer, dist_to_school, dist_to_commerce, dist_to_PPR, MedHHInc, pctForeign, pctMobility))

PhillyPrice.test.nhood <-
  PhillyPrice.test %>%
  mutate(Regression = "Neighborhood Effects",  
         sale_price.Predict = predict(reg.nhood, PhillyPrice.test),
         sale_price.Error = sale_price.Predict- sale_price,
         sale_price.AbsError = abs(sale_price.Predict- sale_price),
         sale_price.APE = (abs(sale_price.Predict- sale_price)) / sale_price)

mape_neighborhood <- PhillyPrice.test.nhood %>%
  group_by(name) %>%
  summarize(MAPE = mean(sale_price.APE, na.rm = TRUE))


neighborhood <- st_join(neighborhood, mape_neighborhood, by = "name")
ggplot(data = neighborhood) +
  geom_sf(aes(fill = MAPE)) +
  scale_fill_gradient(low = "lightblue", high = "darkblue", name = "MAPE") +
  labs(title = "Mean Absolute Percentage Error by Neighborhood") +
  theme_minimal()
```


### Provide a scatterplot plot of MAPE by neighborhood as a function of mean price by neighborhood.
```{r}
mean_price_summary <- PhillyPrice.test.nhood %>%
  group_by(name) %>%
  summarize(meanPrice = mean(sale_price, na.rm = TRUE))

neighborhood <- st_join(neighborhood, mean_price_summary, by = "name")

ggplot(data = neighborhood, aes(x = meanPrice, y = MAPE)) +
  geom_point(alpha = 0.7) +  # color by neighborhood for distinction
  geom_smooth(method = "lm", se = FALSE, color = "red", linetype = "dashed") +
  labs(title = "MAPE by Neighborhood as a Function of Mean Price") +
  theme_minimal()
```
### split your study area into two groups (perhaps by race or income) and test your model’s generalizability
```{r}
blockgroup21 <- blockgroup21 %>%
  mutate(ForeignContext = ifelse(pctForeign > .1, "Majority Foreign-born", "Majority Non-Foreign-born"),
         incomeContext = ifelse(MedHHInc > 65569.4, "High Income", "Low Income"))
  
grid.arrange(ncol = 2,
  ggplot() + geom_sf(data = na.omit(blockgroup21), aes(fill = ForeignContext)) +
    scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Foreign-born Context") +
    labs(title = "Foreign-born Context") +
    mapTheme() + theme(legend.position="bottom"), 
  ggplot() + geom_sf(data = na.omit(blockgroup21), aes(fill = incomeContext)) +
    scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Income Context") +
    labs(title = "Income Context") +
    mapTheme() + theme(legend.position="bottom"))
```
```{r}
# B02001_001 white alone

PhillyPrice.test <- PhillyPrice.test %>%
  mutate(Regression = "Baseline Regression")
# #dplyr::select(PhillyPrice.test, starts_with("sale_price"), Regression, name)
# dplyr::select(PhillyPrice.test.nhood, starts_with("sale_price"), Regression, name)
bothRegressions <-
  rbind(
    dplyr::select(PhillyPrice.test, starts_with("sale_price"), Regression, name),
    dplyr::select(PhillyPrice.test.nhood, starts_with("sale_price"), Regression, name))

st_join(bothRegressions, blockgroup21) %>% 
  group_by(Regression, ForeignContext) %>%
  summarize(mean.MAPE = scales::percent(mean(sale_price.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(ForeignContext, mean.MAPE) %>%
  kable("html", caption = "Test set MAPE by neighborhood racial context") %>%
  kable_styling(full_width = F)
```
```{r}
st_join(bothRegressions, blockgroup21) %>% 
  group_by(Regression, incomeContext) %>%
  summarize(mean.MAPE = scales::percent(mean(sale_price.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(incomeContext, mean.MAPE) %>%
  kable("html", caption = "Test set MAPE by neighborhood racial context") %>%
  kable_styling(full_width = F)
```

